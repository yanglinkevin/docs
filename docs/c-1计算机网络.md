#1. bio nio select poll epoll
阻塞io 就是说 accept 或者 receiv的时候，如果客户端没准备好，那就会阻塞住。所以需要 一个客户端对应一个线程去处理。  非阻塞的话，就是说，可以用一个线程去处理这么多的连接。每次都去循环遍历这些文件描述符，如果有一万个链接，那就是一万次系统调用，非常耗时。 
tips： 阻塞的时候不占用CPU，CPU已经就交出去了。 非阻塞比如Whie(1)这个是一直占用CPU的。
https://juejin.im/post/5eb8e318f265da7bbd2f9339
多路复用，就是说 会有一个线程，去监控这些文件描述符，当其中有准备好的，就再进行读取就可以了。不用遍历，是事件驱动的。
select 就是 每次都要从用户态把 所有的文件描述符传到内核态，然后内核态再遍历这些文件描述符，把所有准备好的 文件描述符返回给用户态。 用户态再进行读写操作。 注意： 这里是内核态遍历o（n），而非阻塞io是发生了 o（n）次的系统调用。   而epoll是在内核态维护了一个数据结构，每次增加或者删除 文件描述符 只传一次给内核态就可以了！ 并且 内核态不需要每次都遍历所有的文件描述符，基于事件驱动，这些文件描述符 有哪个准备好了，会挂载到另一个链表数据结构上！ 用户进程再去链表中去拿准备好的文件描述符就可以了！！！！

简而言之， select 就是每次把fdset（rset， 一个bitmap 1024位）传到内核态。然后内核态进行遍历，看对应的fd是否有事件到达，当没有的时候，就会阻塞住，select会阻塞在那一行，内核态会一直在里面进行判断。让有了一个或者多个事件到达，把对应rset置位并立马返回，这个时候 用户态再去遍历一次 fdset对应的rset是否被置位，若置位了，则进行对应的读写操作。之后，再把fdset对应的rset重新置位回原来的样子，然后再传到内核态。

poll的话 和select区别就是 少了1024限制，然后不需要再遍历一次 将fdset重新置位了，而是在读处理的时候 直接将reevents置回0了。

epoll的高效：
epoll的高效就在于，当我们调用epoll_ctl往里塞入百万个句柄时，epoll_wait仍然可以飞快的返回，并有效的将发生事件的句柄给我们用户。这是由于我们在调用epoll_create时，内核除了帮我们在epoll文件系统里建了个file结点，在内核cache里建了个红黑树用于存储以后epoll_ctl传来的socket外，还会再建立一个list链表，用于存储准备就绪的事件，当epoll_wait调用时，仅仅观察这个list链表里有没有数据即可。有数据就返回，没有数据就sleep，等到timeout时间到后即使链表没数据也返回。所以，epoll_wait非常高效。
就绪list链表维护：
那么，这个准备就绪list链表是怎么维护的呢？当我们执行epoll_ctl时，除了把socket放到epoll文件系统里file对象对应的红黑树上之外，还会给内核中断处理程序注册一个回调函数，告诉内核，如果这个句柄的中断到了，就把它放到准备就绪list链表里。所以，当一个socket上有数据到了，内核在把网卡上的数据copy到内核中后就来把socket插入到准备就绪链表里了。
如此，一颗红黑树，一张准备就绪句柄链表，少量的内核cache，就帮我们解决了大并发下的socket处理问题。执行epoll_create时，创建了红黑树和就绪链表，执行epoll_ctl时，如果增加socket句柄，则检查在红黑树中是否存在，存在立即返回，不存在则添加到树干上，然后向内核注册回调函数，用于当中断事件来临时向准备就绪链表中插入数据。执行epoll_wait时立刻返回准备就绪链表里的数据即可。
两种模式LT和ET：
最后看看epoll独有的两种模式LT和ET。无论是LT和ET模式，都适用于以上所说的流程。区别是，LT模式下，只要一个句柄上的事件一次没有处理完，会在以后调用epoll_wait时次次返回这个句柄，而ET模式仅在第一次返回。
这件事怎么做到的呢？当一个socket句柄上有事件时，内核会把该句柄插入上面所说的准备就绪list链表，这时我们调用epoll_wait，会把准备就绪的socket拷贝到用户态内存，然后清空准备就绪list链表，最后，epoll_wait干了件事，就是检查这些socket，如果不是ET模式（就是LT模式的句柄了），并且这些socket上确实有未处理的事件时，又把该句柄放回到刚刚清空的准备就绪链表了。所以，非ET的句柄，只要它上面还有事件，epoll_wait每次都会返回。而ET模式的句柄，除非有新中断到，即使socket上的事件没有处理完，也是不会次次从epoll_wait返回的。

####实例代码
define MAX_EVENTS 10
int main() {
	struct epoll_event ev, events[MAX_EVENTS];
    int listen_sock, conn_sock, nfds, epollfd;

    /* Code to set up listening socket, 'listen_sock',
     (socket(), bind(), listen()) omitted */

	epollfd = epoll_create1(0);
	if (epollfd == -1) {
		perror("epoll_create1");
      	exit(EXIT_FAILURE);
	}

	ev.events = EPOLLIN;
	ev.data.fd = listen_sock;
	if (epoll_ctl(epollfd, EPOLL_CTL_ADD, listen_sock, &ev) == -1) {
		perror("epoll_ctl: listen_sock");
		exit(EXIT_FAILURE);
	}

	for (;;) {
	    // 永久阻塞，直到有事件
		nfds = epoll_wait(epollfd, events, MAX_EVENTS, -1);
		if (nfds == -1) {  // 处理错误
			perror("epoll_wait");
			exit(EXIT_FAILURE);
		}

		for (n = 0; n < nfds; ++n) {
			if (events[n].data.fd == listen_sock) {
				conn_sock = accept(listen_sock, (struct sockaddr *) &addr, &addrlen);
				if (conn_sock == -1) {
					perror("accept");
					exit(EXIT_FAILURE);
				}
				setnonblocking(conn_sock);
				ev.events = EPOLLIN | EPOLLET;
				ev.data.fd = conn_sock;
				if (epoll_ctl(epollfd, EPOLL_CTL_ADD, conn_sock, &ev) == -1) {
					perror("epoll_ctl: conn_sock");
					exit(EXIT_FAILURE);
				}
			} else {
				do_use_fd(events[n].data.fd);
			}
		}
	}
	return 0;
}
####

